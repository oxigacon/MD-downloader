#!/usr/bin/env python3
"""
Web to Markdown Parser
–ü–∞—Ä—Å–∏—Ç –≤–µ–±-—Å–∞–π—Ç—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–∏–π
"""

import os
import re
import hashlib
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from tqdm import tqdm


class WebToMarkdownParser:
    def __init__(self, base_dir="D:\\Downloads\\MD downloads"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def sanitize_filename(self, name):
        """–û—á–∏—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip('. ')
        return name[:200] if name else 'unnamed'

    def get_site_folder_name(self, url):
        """–°–æ–∑–¥–∞–µ—Ç –∏–º—è –ø–∞–ø–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        path = parsed.path.strip('/').replace('/', '_')

        if path:
            folder_name = f"{domain}_{path}"
        else:
            folder_name = domain

        return self.sanitize_filename(folder_name)

    def download_file(self, url, save_path):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        try:
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()

            total_size = int(response.headers.get('content-length', 0))

            with open(save_path, 'wb') as f:
                if total_size == 0:
                    f.write(response.content)
                else:
                    with tqdm(total=total_size, unit='B', unit_scale=True,
                              desc=save_path.name, leave=False) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
            return True
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return False

    def get_file_extension(self, url, content_type=None):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        parsed = urlparse(url)
        path = unquote(parsed.path)

        if '.' in path:
            ext = os.path.splitext(path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.ico',
                       '.css', '.js', '.pdf', '.mp4', '.webm']:
                return ext

        if content_type:
            ext_map = {
                'image/jpeg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/svg+xml': '.svg',
                'image/webp': '.webp',
                'text/css': '.css',
                'application/javascript': '.js',
                'application/pdf': '.pdf',
            }
            return ext_map.get(content_type, '')

        return ''

    def download_resource(self, url, base_url, assets_dir, resource_type='image'):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, CSS, JS –∏ —Ç.–¥.)"""
        try:
            full_url = urljoin(base_url, url)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ data URI
            if full_url.startswith('data:'):
                return None

            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            url_hash = hashlib.md5(full_url.encode()).hexdigest()[:8]

            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            parsed = urlparse(full_url)
            original_name = os.path.basename(unquote(parsed.path))

            if not original_name or '.' not in original_name:
                response = self.session.head(full_url, timeout=10)
                content_type = response.headers.get('content-type', '').split(';')[0]
                ext = self.get_file_extension(full_url, content_type)
                filename = f"{resource_type}_{url_hash}{ext}"
            else:
                name, ext = os.path.splitext(original_name)
                filename = f"{self.sanitize_filename(name)}_{url_hash}{ext}"

            save_path = assets_dir / filename

            if save_path.exists():
                return filename

            if self.download_file(full_url, save_path):
                return filename

        except Exception as e:
            print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url}: {e}")

        return None

    def process_html_to_markdown(self, html_content, base_url, assets_dir):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–ª–æ–∂–µ–Ω–∏–π"""
        soup = BeautifulSoup(html_content, 'html.parser')

        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∞—Å—Å–µ—Ç–æ–≤
        assets_dir.mkdir(exist_ok=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = soup.find_all('img')
        print(f"üì∑ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")

        for img in tqdm(images, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", unit="img"):
            src = img.get('src') or img.get('data-src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'image')
                if filename:
                    img['src'] = f"assets/{filename}"
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º alt —Ç–µ–∫—Å—Ç
                    if not img.get('alt'):
                        img['alt'] = filename

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS
        css_links = soup.find_all('link', rel='stylesheet')
        print(f"üé® –ù–∞–π–¥–µ–Ω–æ CSS —Ñ–∞–π–ª–æ–≤: {len(css_links)}")

        for link in tqdm(css_links, desc="–ó–∞–≥—Ä—É–∑–∫–∞ CSS", unit="file", leave=False):
            href = link.get('href')
            if href:
                filename = self.download_resource(href, base_url, assets_dir, 'css')
                if filename:
                    link['href'] = f"assets/{filename}"

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã
        scripts = soup.find_all('script', src=True)
        print(f"üìú –ù–∞–π–¥–µ–Ω–æ JS —Ñ–∞–π–ª–æ–≤: {len(scripts)}")

        for script in tqdm(scripts, desc="–ó–∞–≥—Ä—É–∑–∫–∞ JS", unit="file", leave=False):
            src = script.get('src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'js')
                if filename:
                    script['src'] = f"assets/{filename}"

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown
        markdown_content = md(str(soup), heading_style="ATX")

        return markdown_content

    def parse_website(self, url):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        print(f"\nüåê –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")

        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ HTML...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding

            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∞–π—Ç–∞
            folder_name = self.get_site_folder_name(url)
            site_dir = self.base_dir / folder_name
            site_dir.mkdir(exist_ok=True)

            assets_dir = site_dir / 'assets'

            print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {site_dir}")

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
            print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown...")
            markdown_content = self.process_html_to_markdown(
                response.text,
                url,
                assets_dir
            )

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown
            md_file = site_dir / 'content.md'
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(f"# {url}\n\n")
                f.write(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {Path.cwd()}\n\n")
                f.write("---\n\n")
                f.write(markdown_content)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π HTML
            html_file = site_dir / 'original.html'
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)

            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {site_dir}")
            print(f"   üìÑ Markdown: content.md")
            print(f"   üåê HTML: original.html")
            print(f"   üìÇ –ê—Å—Å–µ—Ç—ã: assets/")

        except requests.exceptions.RequestException as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")


def main():
    print("=" * 60)
    print("  üîó Web to Markdown Parser")
    print("=" * 60)

    parser = WebToMarkdownParser()

    while True:
        url = input("\nüîó –í–≤–µ–¥–∏—Ç–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()

        if url.lower() in ['exit', 'quit', 'q']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if not url:
            print("‚ö†Ô∏è  URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            continue

        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        parser.parse_website(url)


if __name__ == "__main__":
    main()
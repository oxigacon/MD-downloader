#!/usr/bin/env python3
"""
Web to Markdown Parser
–ü–∞—Ä—Å–∏—Ç –≤–µ–±-—Å–∞–π—Ç—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–∏–π
"""

import os
import re
import hashlib
import json
import base64
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from tqdm import tqdm
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from webdriver_manager.chrome import ChromeDriverManager


class WebToMarkdownParser:
    def __init__(self, base_dir="D:\\Downloads\\MD downloads"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Selenium –¥–ª—è PDF
        self.driver = None
        self.pdf_available = self.init_selenium()
        
    def init_selenium(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç Selenium WebDriver"""
        try:
            print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Chrome WebDriver...")
            
            chrome_options = Options()
            chrome_options.add_argument('--headless')  # –ë–µ–∑ GUI
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--window-size=1920,1080')
            
            # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ ChromeDriver
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            print("‚úÖ Chrome WebDriver –≥–æ—Ç–æ–≤")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å Selenium: {e}")
            print("üí° PDF –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è")
            return False
    
    def __del__(self):
        """–ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä –ø—Ä–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–∏"""
        if self.driver:
            try:
                self.driver.quit()
            except:
                pass
        
    def sanitize_filename(self, name):
        """–û—á–∏—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip('. ')
        return name[:200] if name else 'unnamed'
    
    def get_site_folder_name(self, url):
        """–°–æ–∑–¥–∞–µ—Ç –∏–º—è –ø–∞–ø–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        path = parsed.path.strip('/').replace('/', '_')
        
        if path:
            folder_name = f"{domain}_{path}"
        else:
            folder_name = domain
            
        return self.sanitize_filename(folder_name)
    
    def download_file(self, url, save_path):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        try:
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(save_path, 'wb') as f:
                if total_size == 0:
                    f.write(response.content)
                else:
                    with tqdm(total=total_size, unit='B', unit_scale=True, 
                             desc=save_path.name, leave=False) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
            return True
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return False
    
    def get_file_extension(self, url, content_type=None):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        parsed = urlparse(url)
        path = unquote(parsed.path)
        
        if '.' in path:
            ext = os.path.splitext(path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.ico', 
                       '.css', '.js', '.pdf', '.mp4', '.webm']:
                return ext
        
        if content_type:
            ext_map = {
                'image/jpeg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/svg+xml': '.svg',
                'image/webp': '.webp',
                'text/css': '.css',
                'application/javascript': '.js',
                'application/pdf': '.pdf',
            }
            return ext_map.get(content_type, '')
        
        return ''
    
    def download_resource(self, url, base_url, assets_dir, resource_type='image'):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, CSS, JS –∏ —Ç.–¥.)"""
        try:
            full_url = urljoin(base_url, url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ data URI
            if full_url.startswith('data:'):
                return None
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            url_hash = hashlib.md5(full_url.encode()).hexdigest()[:8]
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            parsed = urlparse(full_url)
            original_name = os.path.basename(unquote(parsed.path))
            
            if not original_name or '.' not in original_name:
                response = self.session.head(full_url, timeout=10)
                content_type = response.headers.get('content-type', '').split(';')[0]
                ext = self.get_file_extension(full_url, content_type)
                filename = f"{resource_type}_{url_hash}{ext}"
            else:
                name, ext = os.path.splitext(original_name)
                filename = f"{self.sanitize_filename(name)}_{url_hash}{ext}"
            
            save_path = assets_dir / filename
            
            if save_path.exists():
                return filename
            
            if self.download_file(full_url, save_path):
                return filename
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url}: {e}")
        
        return None
    
    def process_html_to_markdown(self, html_content, base_url, assets_dir):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–ª–æ–∂–µ–Ω–∏–π"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∞—Å—Å–µ—Ç–æ–≤
        assets_dir.mkdir(exist_ok=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = soup.find_all('img')
        print(f"üì∑ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        
        for img in tqdm(images, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", unit="img"):
            src = img.get('src') or img.get('data-src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'image')
                if filename:
                    img['src'] = f"assets/{filename}"
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º alt —Ç–µ–∫—Å—Ç
                    if not img.get('alt'):
                        img['alt'] = filename
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS
        css_links = soup.find_all('link', rel='stylesheet')
        print(f"üé® –ù–∞–π–¥–µ–Ω–æ CSS —Ñ–∞–π–ª–æ–≤: {len(css_links)}")
        
        for link in tqdm(css_links, desc="–ó–∞–≥—Ä—É–∑–∫–∞ CSS", unit="file", leave=False):
            href = link.get('href')
            if href:
                filename = self.download_resource(href, base_url, assets_dir, 'css')
                if filename:
                    link['href'] = f"assets/{filename}"
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã
        scripts = soup.find_all('script', src=True)
        print(f"üìú –ù–∞–π–¥–µ–Ω–æ JS —Ñ–∞–π–ª–æ–≤: {len(scripts)}")
        
        for script in tqdm(scripts, desc="–ó–∞–≥—Ä—É–∑–∫–∞ JS", unit="file", leave=False):
            src = script.get('src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'js')
                if filename:
                    script['src'] = f"assets/{filename}"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown
        markdown_content = md(str(soup), heading_style="ATX")
        
        return markdown_content
    
    def generate_pdf(self, url, pdf_path):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç PDF —á–µ—Ä–µ–∑ Chrome Print to PDF"""
        if not self.pdf_available or not self.driver:
            return False
            
        try:
            print("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF —á–µ—Ä–µ–∑ Chrome...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            self.driver.get(url)
            
            # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            WebDriverWait(self.driver, 10).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø–∞—É–∑–∞ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Å—Ç–∏–ª–µ–π
            import time
            time.sleep(2)
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º Chrome DevTools Protocol –¥–ª—è –ø–µ—á–∞—Ç–∏ –≤ PDF
            pdf_data = self.driver.execute_cdp_cmd("Page.printToPDF", {
                "printBackground": True,
                "landscape": False,
                "paperWidth": 8.27,  # A4 width in inches
                "paperHeight": 11.69,  # A4 height in inches
                "marginTop": 0.4,
                "marginBottom": 0.4,
                "marginLeft": 0.4,
                "marginRight": 0.4,
                "displayHeaderFooter": False,
                "preferCSSPageSize": False,
                "generateDocumentOutline": False,
                "generateTaggedPDF": False,
                "transferMode": "ReturnAsBase64"
            })
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º PDF
            with open(pdf_path, 'wb') as f:
                f.write(base64.b64decode(pdf_data['data']))
            
            print(f"‚úÖ PDF —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {pdf_path.name}")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è PDF: {e}")
            return False
    
    def parse_website(self, url):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        print(f"\nüåê –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ HTML...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∞–π—Ç–∞
            folder_name = self.get_site_folder_name(url)
            site_dir = self.base_dir / folder_name
            site_dir.mkdir(exist_ok=True)
            
            assets_dir = site_dir / 'assets'
            
            print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {site_dir}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
            print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown...")
            markdown_content = self.process_html_to_markdown(
                response.text, 
                url, 
                assets_dir
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown
            md_file = site_dir / 'content.md'
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(f"# {url}\n\n")
                f.write(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {Path.cwd()}\n\n")
                f.write("---\n\n")
                f.write(markdown_content)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π HTML
            html_file = site_dir / 'original.html'
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF —á–µ—Ä–µ–∑ Chrome (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            pdf_file = site_dir / 'page.pdf'
            pdf_created = self.generate_pdf(url, pdf_file)
            
            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {site_dir}")
            print(f"   üìÑ Markdown: content.md")
            print(f"   üåê HTML: original.html")
            if pdf_created:
                print(f"   üìï PDF: page.pdf")
            print(f"   üìÇ –ê—Å—Å–µ—Ç—ã: assets/")
            
        except requests.exceptions.RequestException as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")


def main():
    print("=" * 60)
    print("  üîó Web to Markdown Parser (with Chrome PDF)")
    print("=" * 60)
    
    parser = WebToMarkdownParser()
    
    if not parser.pdf_available:
        print("\n" + "=" * 60)
        print("  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: Selenium –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
        print("  üìù PDF —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç")
        print("  üí° –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install selenium webdriver-manager")
        print("=" * 60 + "\n")
    
    try:
        while True:
            url = input("\nüîó –í–≤–µ–¥–∏—Ç–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
            
            if url.lower() in ['exit', 'quit', 'q']:
                print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
                break
            
            if not url:
                print("‚ö†Ô∏è  URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
                continue
            
            if not url.startswith(('http://', 'https://')):
                url = 'https://' + url
            
            parser.parse_website(url)
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –¥—Ä–∞–π–≤–µ—Ä –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ
        if parser.driver:
            parser.driver.quit()


if __name__ == "__main__":
    main()
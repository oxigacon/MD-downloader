#!/usr/bin/env python3
"""
Web to Markdown Parser
–ü–∞—Ä—Å–∏—Ç –≤–µ–±-—Å–∞–π—Ç—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –∏—Ö –≤ Markdown —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤—Å–µ—Ö –≤–ª–æ–∂–µ–Ω–∏–π
"""

import os
import re
import hashlib
from urllib.parse import urljoin, urlparse, unquote
from pathlib import Path
import requests
from bs4 import BeautifulSoup
from markdownify import markdownify as md
from tqdm import tqdm
import pdfkit

class WebToMarkdownParser:
    def __init__(self, base_dir="D:\\Downloads\\MD downloads"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ wkhtmltopdf
        self.pdf_available = self.check_wkhtmltopdf()
        
    def check_wkhtmltopdf(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ wkhtmltopdf"""
        try:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ wkhtmltopdf
            pdfkit.configuration()
            return True
        except Exception:
            print("‚ö†Ô∏è  wkhtmltopdf –Ω–µ –Ω–∞–π–¥–µ–Ω. PDF –Ω–µ –±—É–¥–µ—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è.")
            print("üí° –°–∫–∞—á–∞–π—Ç–µ —Å: https://wkhtmltopdf.org/downloads.html")
            return False
        
    def sanitize_filename(self, name):
        """–û—á–∏—â–∞–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤"""
        name = re.sub(r'[<>:"/\\|?*]', '_', name)
        name = name.strip('. ')
        return name[:200] if name else 'unnamed'
    
    def get_site_folder_name(self, url):
        """–°–æ–∑–¥–∞–µ—Ç –∏–º—è –ø–∞–ø–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ URL"""
        parsed = urlparse(url)
        domain = parsed.netloc.replace('www.', '')
        path = parsed.path.strip('/').replace('/', '_')
        
        if path:
            folder_name = f"{domain}_{path}"
        else:
            folder_name = domain
            
        return self.sanitize_filename(folder_name)
    
    def download_file(self, url, save_path):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–æ–º"""
        try:
            response = self.session.get(url, stream=True, timeout=30)
            response.raise_for_status()
            
            total_size = int(response.headers.get('content-length', 0))
            
            with open(save_path, 'wb') as f:
                if total_size == 0:
                    f.write(response.content)
                else:
                    with tqdm(total=total_size, unit='B', unit_scale=True, 
                             desc=save_path.name, leave=False) as pbar:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                pbar.update(len(chunk))
            return True
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return False
    
    def get_file_extension(self, url, content_type=None):
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        parsed = urlparse(url)
        path = unquote(parsed.path)
        
        if '.' in path:
            ext = os.path.splitext(path)[1].lower()
            if ext in ['.jpg', '.jpeg', '.png', '.gif', '.svg', '.webp', '.ico', 
                       '.css', '.js', '.pdf', '.mp4', '.webm']:
                return ext
        
        if content_type:
            ext_map = {
                'image/jpeg': '.jpg',
                'image/png': '.png',
                'image/gif': '.gif',
                'image/svg+xml': '.svg',
                'image/webp': '.webp',
                'text/css': '.css',
                'application/javascript': '.js',
                'application/pdf': '.pdf',
            }
            return ext_map.get(content_type, '')
        
        return ''
    
    def download_resource(self, url, base_url, assets_dir, resource_type='image'):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ä–µ—Å—É—Ä—Å (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, CSS, JS –∏ —Ç.–¥.)"""
        try:
            full_url = urljoin(base_url, url)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ data URI
            if full_url.startswith('data:'):
                return None
            
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            url_hash = hashlib.md5(full_url.encode()).hexdigest()[:8]
            
            # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
            parsed = urlparse(full_url)
            original_name = os.path.basename(unquote(parsed.path))
            
            if not original_name or '.' not in original_name:
                response = self.session.head(full_url, timeout=10)
                content_type = response.headers.get('content-type', '').split(';')[0]
                ext = self.get_file_extension(full_url, content_type)
                filename = f"{resource_type}_{url_hash}{ext}"
            else:
                name, ext = os.path.splitext(original_name)
                filename = f"{self.sanitize_filename(name)}_{url_hash}{ext}"
            
            save_path = assets_dir / filename
            
            if save_path.exists():
                return filename
            
            if self.download_file(full_url, save_path):
                return filename
            
        except Exception as e:
            print(f"\n‚ö†Ô∏è  –ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å {url}: {e}")
        
        return None
    
    def process_html_to_markdown(self, html_content, base_url, assets_dir):
        """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç HTML –≤ Markdown —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –≤–ª–æ–∂–µ–Ω–∏–π"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –∞—Å—Å–µ—Ç–æ–≤
        assets_dir.mkdir(exist_ok=True)
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        images = soup.find_all('img')
        print(f"üì∑ –ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(images)}")
        
        for img in tqdm(images, desc="–ó–∞–≥—Ä—É–∑–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π", unit="img"):
            src = img.get('src') or img.get('data-src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'image')
                if filename:
                    img['src'] = f"assets/{filename}"
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º alt —Ç–µ–∫—Å—Ç
                    if not img.get('alt'):
                        img['alt'] = filename
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ CSS
        css_links = soup.find_all('link', rel='stylesheet')
        print(f"üé® –ù–∞–π–¥–µ–Ω–æ CSS —Ñ–∞–π–ª–æ–≤: {len(css_links)}")
        
        for link in tqdm(css_links, desc="–ó–∞–≥—Ä—É–∑–∫–∞ CSS", unit="file", leave=False):
            href = link.get('href')
            if href:
                filename = self.download_resource(href, base_url, assets_dir, 'css')
                if filename:
                    link['href'] = f"assets/{filename}"
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–∫—Ä–∏–ø—Ç—ã
        scripts = soup.find_all('script', src=True)
        print(f"üìú –ù–∞–π–¥–µ–Ω–æ JS —Ñ–∞–π–ª–æ–≤: {len(scripts)}")
        
        for script in tqdm(scripts, desc="–ó–∞–≥—Ä—É–∑–∫–∞ JS", unit="file", leave=False):
            src = script.get('src')
            if src:
                filename = self.download_resource(src, base_url, assets_dir, 'js')
                if filename:
                    script['src'] = f"assets/{filename}"
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Markdown
        markdown_content = md(str(soup), heading_style="ATX")
        
        return markdown_content, str(soup)
    
    def generate_pdf(self, html_content, url, pdf_path, site_dir):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç PDF –∏–∑ HTML —Å –ø–æ–º–æ—â—å—é wkhtmltopdf"""
        if not self.pdf_available:
            return False
            
        try:
            print("üìÑ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è PDF...")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π HTML —Ñ–∞–π–ª
            temp_html = site_dir / 'temp_for_pdf.html'
            with open(temp_html, 'w', encoding='utf-8') as f:
                f.write(html_content)
            
            # –û–ø—Ü–∏–∏ –¥–ª—è wkhtmltopdf
            options = {
                'page-size': 'A4',
                'margin-top': '20mm',
                'margin-right': '20mm',
                'margin-bottom': '20mm',
                'margin-left': '20mm',
                'encoding': 'UTF-8',
                'enable-local-file-access': None,
                'no-stop-slow-scripts': None,
                'quiet': '',
            }
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF
            pdfkit.from_file(str(temp_html), str(pdf_path), options=options)
            
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            temp_html.unlink()
            
            print(f"‚úÖ PDF —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {pdf_path.name}")
            return True
            
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è PDF: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ wkhtmltopdf —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω–æ")
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if temp_html.exists():
                temp_html.unlink()
            return False
    
    def parse_website(self, url):
        """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∞–π—Ç–∞"""
        print(f"\nüåê –ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥: {url}")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ HTML...")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Å–∞–π—Ç–∞
            folder_name = self.get_site_folder_name(url)
            site_dir = self.base_dir / folder_name
            site_dir.mkdir(exist_ok=True)
            
            assets_dir = site_dir / 'assets'
            
            print(f"üìÅ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤: {site_dir}")
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML –≤ Markdown
            print("üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown...")
            markdown_content, processed_html = self.process_html_to_markdown(
                response.text, 
                url, 
                assets_dir
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º Markdown
            md_file = site_dir / 'content.md'
            with open(md_file, 'w', encoding='utf-8') as f:
                f.write(f"# {url}\n\n")
                f.write(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {Path.cwd()}\n\n")
                f.write("---\n\n")
                f.write(markdown_content)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π HTML
            html_file = site_dir / 'original.html'
            with open(html_file, 'w', encoding='utf-8') as f:
                f.write(response.text)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º PDF (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω–æ)
            pdf_file = site_dir / 'page.pdf'
            pdf_created = self.generate_pdf(processed_html, url, pdf_file, site_dir)
            
            print(f"\n‚úÖ –ì–æ—Ç–æ–≤–æ! –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {site_dir}")
            print(f"   üìÑ Markdown: content.md")
            print(f"   üåê HTML: original.html")
            if pdf_created:
                print(f"   üìï PDF: page.pdf")
            print(f"   üìÇ –ê—Å—Å–µ—Ç—ã: assets/")
            
        except requests.exceptions.RequestException as e:
            print(f"\n‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {e}")
        except Exception as e:
            print(f"\n‚ùå –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {e}")


def main():
    print("=" * 60)
    print("  üîó Web to Markdown Parser (with PDF)")
    print("=" * 60)
    
    parser = WebToMarkdownParser()
    
    if not parser.pdf_available:
        print("\n" + "=" * 60)
        print("  ‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: wkhtmltopdf –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        print("  üìù PDF —Ñ–∞–π–ª—ã —Å–æ–∑–¥–∞–≤–∞—Ç—å—Å—è –Ω–µ –±—É–¥—É—Ç")
        print("  üí° –°–∫–∞—á–∞–π—Ç–µ —Å: https://wkhtmltopdf.org/downloads.html")
        print("=" * 60 + "\n")
    
    while True:
        url = input("\nüîó –í–≤–µ–¥–∏—Ç–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞): ").strip()
        
        if url.lower() in ['exit', 'quit', 'q']:
            print("üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break
        
        if not url:
            print("‚ö†Ô∏è  URL –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
            continue
        
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        parser.parse_website(url)


if __name__ == "__main__":
    main()